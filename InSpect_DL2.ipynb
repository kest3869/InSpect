{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading 2: TFRecords Object \n",
    "We need to express our dataset as a TFRecords object which is what the nobrainer models use. Since we are going to be using transfer learning to train them, our dataset format has to be compatible with what they use. It has the added benifit of being easily expressed as text files, which we need since our TF-GPU kernel is not compatible with our nobrainer kernel. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT Function \n",
    "the nobrainer library has a function to convert the dataset from a csv format to a tfRecords object. Before I dive into implementing that, I wanted to see if chatGPT could come up with a working adaptation of the function I was already using. Below is what it gave me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the Above Code\n",
    "It's a mess, and frankly I have no idea if its complete / works. I am stubborn, so before moving on I wanted to try instead having it just return an unsplit dataset object, then have it modify that function to return a tfrecords object. Here is what I got: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(feature0, feature1):\n",
    "    \"\"\"\n",
    "    Creates a tf.train.Example message from two input features.\n",
    "    \"\"\"\n",
    "    feature = {\n",
    "        'data': tf.train.Feature(float_list=tf.train.FloatList(value=feature0.flatten())),\n",
    "        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[feature1])),\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def load_data_old(top_dir):\n",
    "    # Get a list of all the file paths and labels\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    for i, folder in enumerate(os.listdir(top_dir)):\n",
    "        for file in os.listdir(os.path.join(top_dir, folder)):\n",
    "            if 'max' in str(file):\n",
    "                file_paths.append(os.path.join(top_dir, folder, file))\n",
    "                labels.append(i)\n",
    "\n",
    "    # Load the images into a numpy array\n",
    "    data = []\n",
    "    for filename in file_paths:\n",
    "        img = nib.load(filename)\n",
    "        data.append(img.get_fdata())\n",
    "    data = np.array(data)\n",
    "\n",
    "    # Convert labels to numpy array\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Reassign labels assigned to 2 to 0\n",
    "    labels[labels==2] = 0\n",
    "\n",
    "    # Serialize the numpy arrays into a tfrecords file\n",
    "    with tf.io.TFRecordWriter('data.tfrecords') as writer:\n",
    "        for d, l in zip(data, labels):\n",
    "            serialized_example = serialize_example(d, l)\n",
    "            writer.write(serialized_example)\n",
    "\n",
    "    # Create a tf.data.Dataset from the tfrecords file\n",
    "    dataset = tf.data.TFRecordDataset('data.tfrecords')\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty Good \n",
    "That seems like it should work pretty nicely. Now I just need to figure out a reasonable way to test it out to make sure that its working the way I think it should be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\inspect\\\\InSpect\\\\datasets'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_data = os.getcwd()+'\\datasets'\n",
    "path_to_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data_old(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(), dtype=tf.string, name=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.element_spec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Could Work but Not Ideal \n",
    "The problem with this is that the data is not split the way I want it to be, and I also won't be able to fit a lot of these models in memory without splitting. So it looks I will have to do things the \"right way\". Let's start the process of learning how to use this nobrainer library to build models these models. Learning how to use nobrainer will be good experience anyways so the discomfort will be worth the price. We will start by looking at this tutorial \"train_binary_classification.ipynb\" to take a look at exactly how this input data needs to be formatted. Then we will return to the data loading tutorial, except we will follow along with our SPECT scan code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nobrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "csv_of_filepaths = nobrainer.utils.get_data() \n",
    "filepaths = nobrainer.io.read_csv(csv_of_filepaths)\n",
    "\n",
    "# Add random boolean values (our labels)\n",
    "filepaths = [(x, random.choice([0, 1])) for x, _ in filepaths]\n",
    "\n",
    "train_paths = filepaths[:9]\n",
    "evaluate_paths = filepaths[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C:\\\\Users\\\\inspect\\\\AppData\\\\Local\\\\Temp\\\\nobrainer-data\\\\datasets\\\\sub-01_t1.mgz',\n",
       "  0),\n",
       " ('C:\\\\Users\\\\inspect\\\\AppData\\\\Local\\\\Temp\\\\nobrainer-data\\\\datasets\\\\sub-02_t1.mgz',\n",
       "  1),\n",
       " ('C:\\\\Users\\\\inspect\\\\AppData\\\\Local\\\\Temp\\\\nobrainer-data\\\\datasets\\\\sub-03_t1.mgz',\n",
       "  0),\n",
       " ('C:\\\\Users\\\\inspect\\\\AppData\\\\Local\\\\Temp\\\\nobrainer-data\\\\datasets\\\\sub-04_t1.mgz',\n",
       "  0),\n",
       " ('C:\\\\Users\\\\inspect\\\\AppData\\\\Local\\\\Temp\\\\nobrainer-data\\\\datasets\\\\sub-05_t1.mgz',\n",
       "  0),\n",
       " ('C:\\\\Users\\\\inspect\\\\AppData\\\\Local\\\\Temp\\\\nobrainer-data\\\\datasets\\\\sub-06_t1.mgz',\n",
       "  1),\n",
       " ('C:\\\\Users\\\\inspect\\\\AppData\\\\Local\\\\Temp\\\\nobrainer-data\\\\datasets\\\\sub-07_t1.mgz',\n",
       "  1),\n",
       " ('C:\\\\Users\\\\inspect\\\\AppData\\\\Local\\\\Temp\\\\nobrainer-data\\\\datasets\\\\sub-08_t1.mgz',\n",
       "  0),\n",
       " ('C:\\\\Users\\\\inspect\\\\AppData\\\\Local\\\\Temp\\\\nobrainer-data\\\\datasets\\\\sub-09_t1.mgz',\n",
       "  0),\n",
       " ('C:\\\\Users\\\\inspect\\\\AppData\\\\Local\\\\Temp\\\\nobrainer-data\\\\datasets\\\\sub-10_t1.mgz',\n",
       "  1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nobrainer Input Shape \n",
    "So I am dumb because I wasn't rock solid on how csv's work. This is actually a pretty trivial problem to solve since my original data loading function already generates a numpy array and finds all the filepaths. I just need to modify it a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_data(top_dir):\n",
    "\n",
    "\n",
    "    \n",
    "    # Get a list of all the file paths and labels\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    for i, folder in enumerate(os.listdir(top_dir)):\n",
    "        for file in os.listdir(os.path.join(top_dir, folder)):\n",
    "            if 'max' in str(file):\n",
    "                file_paths.append(os.path.join(top_dir, folder, file))\n",
    "                labels.append(i)\n",
    "\n",
    "    # Reassign labels assigned to 2 to 0\n",
    "    labels = np.array(labels)\n",
    "    labels[labels==2] = 0\n",
    "\n",
    "    # Create a list of tuples with file paths and labels\n",
    "    data = list(zip(file_paths, labels))\n",
    "\n",
    "    # Save the data to a .csv file\n",
    "    with open('dataCsv.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['path', 'label'])\n",
    "        writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\inspect\\\\InSpect\\\\datasets'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get path to data \n",
    "path_to_data = os.getcwd()+'\\datasets'\n",
    "path_to_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function to make the CSV \n",
    "load_data(path_to_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results \n",
    "Inspection of the CSV confirms that the function is working as intended. Now we should be able to follow along with the tutorial for data loading using our own dataset! Even though this format isn't technically allowed by this tutorial (the tutorial allows for data, label or pathtodata, pathtolabel) a later tutorial uses this format so I think it should be fine. Let's press on! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 95, 79)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the correct shape \n",
    "data_path = os.getcwd() + '/datasets/ptsd/'\n",
    "example_filename = os.path.join(data_path, 'sub-1928_ses-rest_spect_MNI_max.nii')\n",
    "img = nib.load(example_filename)\n",
    "shape = img.shape\n",
    "shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharding: Breaking up our dataset based on its size\n",
    "Accoding to this the tensorflow documentation. https://www.tensorflow.org/tutorials/load_data/tfrecord . We need to shard our dataset to take full advantage of this optimization. We will be using 1 host so that means we want about **10 shards**. Ideally these shards would be greater than 100 mb in size. The tfrecord we made with our naive implementation earlier produced a file with a size of 2.3 gb, so each shard should be about 230 mb, so we are good to go! Since the function takes number of exmaples per shard we just divide our dataset size by num_shards to get examples per shard. (this will usually get us 11 shards, but 10 of them will be optimally sized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_shards = 10\n",
    "dataset_size = 1033\n",
    "examples_per_shard = dataset_size // num_shards \n",
    "examples_per_shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\inspect\\\\InSpect\\\\dataCsv.csv'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we save our path to csv \n",
    "path_to_csv = str(os.getcwd() + '\\dataCsv.csv')\n",
    "path_to_csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Object \n",
    "If we have everything set up correctly, we should be able to call the function using the parameters we defined above. Checking that all our stull makes sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying 9 examples\n",
      "9/9 [==============================] - 4s 749us/step\n",
      "Verifying 1 examples\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    }
   ],
   "source": [
    "filepaths = nobrainer.io.read_csv(path_to_csv)\n",
    "invalid = nobrainer.io.verify_features_labels(train_paths)\n",
    "assert not invalid\n",
    "\n",
    "invalid = nobrainer.io.verify_features_labels(evaluate_paths)\n",
    "assert not invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\inspect\\\\InSpect\\\\dataCsv.csv'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C:\\Users\\inspect\\InSpect\\dataCsv.csv\n",
    "path_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: nobrainer convert [OPTIONS]\n",
      "Try 'nobrainer convert --help' for help.\n",
      "\n",
      "Error: Invalid value for '-c' / '--csv': Path \"'c:\\\\Users\\\\inspect\\\\InSpect\\\\dataCsv.csv'\" does not exist.\n"
     ]
    }
   ],
   "source": [
    "!nobrainer convert \\\n",
    "    --csv='c:\\Users\\inspect\\InSpect\\dataCsv.csv' \\\n",
    "    --tfrecords-template='data/data_shard-{shard:03d}.tfrec' \\\n",
    "    --examples-per-shard=103 \\\n",
    "    --volume-shape=79 95 79 \\\n",
    "    --verbose\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nobrainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
