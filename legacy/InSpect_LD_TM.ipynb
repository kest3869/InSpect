{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Model Testing\n",
    "This is a streamlined file for quickly loading data and testing out some models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf \n",
    "import os \n",
    "import pyvista as pv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import ndimage\n",
    "import random \n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for GPU Support "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(top_dir, test_size=0.2, val_size=0.1):\n",
    "    # Get a list of all the file paths\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    for i, folder in enumerate(os.listdir(top_dir)):\n",
    "        for file in os.listdir(top_dir + '/' + str(folder)):\n",
    "            if 'max' in str(file):\n",
    "                file_paths.append(os.path.join(top_dir, folder, file))\n",
    "                labels.append(i)\n",
    "\n",
    "    # Load the images into a numpy array\n",
    "    data = []\n",
    "    for filename in file_paths:\n",
    "        img = nib.load(filename)\n",
    "        data.append(img.get_fdata())\n",
    "    data = np.array(data)\n",
    "\n",
    "    # Convert labels to numpy array\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Reassign labels assigned to 2 to 0\n",
    "    labels[labels==2] = 0\n",
    "\n",
    "    # Split the data into train, validation, and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=test_size, stratify=labels)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, stratify=y_train)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Loading Data!\n"
     ]
    }
   ],
   "source": [
    "# Loading our data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = load_data('../InSpect/datasets')\n",
    "print(\"Finished Loading Data!\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing With GPU Issues \n",
    "For some reason, if I try to load the following dataset to memory I can't, so I'll try limiting the memory to see if that works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngpus = tf.config.experimental.list_physical_devices(\\'GPU\\')\\nif gpus:\\n    # Restrict TensorFlow to only allocate 2GB * 2 of memory on the first GPU\\n    try:\\n        tf.config.experimental.set_virtual_device_configuration(\\n            gpus[0],\\n            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048 * 2)])\\n        logical_gpus = tf.config.experimental.list_logical_devices(\\'GPU\\')\\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\\n    except RuntimeError as e:\\n        # Virtual devices must be set before GPUs have been initialized\\n        print(e)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 2GB * 2 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048 * 2)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train_preprocessing(x, y):\\n    x = tf.image.random_flip_left_right(x)\\n    x = tf.image.resize(x, [256, 256])\\n    x = tf.image.random_crop(x, [224, 224, 3])\\n    x = tf.keras.applications.resnet_v2.preprocess_input(x)\\n    y = tf.one_hot(y, depth=5)\\n    return x, y\\n\\ndef validation_preprocessing(x, y):\\n    x = tf.image.resize(x, [224, 224])\\n    x = tf.keras.applications.resnet_v2.preprocess_input(x)\\n    y = tf.one_hot(y, depth=5)\\n    return x, y\\n\\n\\ntrain_loader = tf.data.Dataset.from_tensor_slices((X_train, y_train))\\nvalidation_loader = tf.data.Dataset.from_tensor_slices((X_val, y_val))\\n\\n#batch_size = 2\\n\\n\\ntrain_dataset = (\\n    train_loader.shuffle(len(X_train))\\n    .map(lambda x, y: train_preprocessing(x, y))\\n    .batch(batch_size)\\n    .prefetch(2)\\n)\\n\\nvalidation_dataset = (\\n    validation_loader.shuffle(len(X_val))\\n    .map(lambda x, y: validation_preprocessing(x, y))\\n    .batch(batch_size)\\n    .prefetch(2)\\n)\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def train_preprocessing(x, y):\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    x = tf.image.resize(x, [256, 256])\n",
    "    x = tf.image.random_crop(x, [224, 224, 3])\n",
    "    x = tf.keras.applications.resnet_v2.preprocess_input(x)\n",
    "    y = tf.one_hot(y, depth=5)\n",
    "    return x, y\n",
    "\n",
    "def validation_preprocessing(x, y):\n",
    "    x = tf.image.resize(x, [224, 224])\n",
    "    x = tf.keras.applications.resnet_v2.preprocess_input(x)\n",
    "    y = tf.one_hot(y, depth=5)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "\n",
    "#batch_size = 2\n",
    "\n",
    "\n",
    "train_dataset = (\n",
    "    train_loader.shuffle(len(X_train))\n",
    "    .map(lambda x, y: train_preprocessing(x, y))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "\n",
    "validation_dataset = (\n",
    "    validation_loader.shuffle(len(X_val))\n",
    "    .map(lambda x, y: validation_preprocessing(x, y))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(width=79, height=95, depth=79):\n",
    "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
    "\n",
    "    inputs = keras.Input((width, height, depth, 1))\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick X-train EDA \n",
    "It doesn't like the way its being passed the training and validation data. Let's see what's in there and try to give it a chance to train on GPU. If it works on this side, that means the gpu data modification has a problem only for data loading. Which we can fix by generating the tf.Dataset object on CPU then saving it in a file for training on the GPU. \n",
    "\n",
    "### Conclusion\n",
    "Oops, since I cut out those earlier functions. I need to re-integrate y-train into the equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(743, 79, 95, 79)\n",
      "(743,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Compiling Model \n",
    "I have now spent a great deal of time and energy getting myself to a point where I now see that my current model will not fit on my GPU, this transfer learning has moved from a potential upside to an absolute necessity. Good thing I was already planning on doing that! On the plus side, since I was already planning on using CPU for my data pre-precessing, I won't have to make any significant changes and I already have everything in the form of a tf.Dataset! Sometimes you gotta take the good with the bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Buld Model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m get_model()\n\u001b[0;32m      4\u001b[0m \u001b[39m# Compile model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m initial_learning_rate \u001b[39m=\u001b[39m \u001b[39m0.0001\u001b[39m\n",
      "Cell \u001b[1;32mIn[31], line 14\u001b[0m, in \u001b[0;36mget_model\u001b[1;34m(width, height, depth)\u001b[0m\n\u001b[0;32m     11\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mMaxPool3D(pool_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)(x)\n\u001b[0;32m     12\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mBatchNormalization()(x)\n\u001b[1;32m---> 14\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39;49mConv3D(filters\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, kernel_size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m\"\u001b[39;49m)(x)\n\u001b[0;32m     15\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mMaxPool3D(pool_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)(x)\n\u001b[0;32m     16\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mBatchNormalization()(x)\n",
      "File \u001b[1;32mc:\\Users\\inspect\\miniconda3\\envs\\tfgpu2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\inspect\\miniconda3\\envs\\tfgpu2\\lib\\site-packages\\keras\\backend.py:2100\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[1;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[0;32m   2098\u001b[0m     \u001b[39mif\u001b[39;00m nonce:\n\u001b[0;32m   2099\u001b[0m         seed \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[1;32m-> 2100\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mstateless_uniform(\n\u001b[0;32m   2101\u001b[0m         shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m   2102\u001b[0m         minval\u001b[39m=\u001b[39;49mminval,\n\u001b[0;32m   2103\u001b[0m         maxval\u001b[39m=\u001b[39;49mmaxval,\n\u001b[0;32m   2104\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   2105\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m   2106\u001b[0m     )\n\u001b[0;32m   2107\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\n\u001b[0;32m   2108\u001b[0m     shape\u001b[39m=\u001b[39mshape,\n\u001b[0;32m   2109\u001b[0m     minval\u001b[39m=\u001b[39mminval,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2112\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_legacy_seed(),\n\u001b[0;32m   2113\u001b[0m )\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul]"
     ]
    }
   ],
   "source": [
    "# Buld Model\n",
    "model = get_model()\n",
    "\n",
    "# Compile model\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "'''\n",
    "# Define callbacks.\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"3d_image_classification.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "'''\n",
    "\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch\n",
    "epochs = 5\n",
    "model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    #validation_data=X_val,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    #callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
